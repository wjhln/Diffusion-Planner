import os
import argparse
import json
import multiprocessing as mp
from functools import partial
from concurrent.futures import ProcessPoolExecutor, as_completed
import time
from tqdm import tqdm
import threading

from diffusion_planner.data_process.data_processor import DataProcessor

from nuplan.planning.utils.multithreading.worker_parallel import SingleMachineParallelExecutor
from nuplan.planning.scenario_builder.scenario_filter import ScenarioFilter
from nuplan.planning.scenario_builder.nuplan_db.nuplan_scenario_builder import NuPlanScenarioBuilder
from nuplan.planning.scenario_builder.nuplan_db.nuplan_scenario_utils import ScenarioMapping

def get_filter_parameters(num_scenarios_per_type=None, limit_total_scenarios=None, shuffle=True, scenario_tokens=None, log_names=None):

    scenario_types = None

    scenario_tokens                      # List of scenario tokens to include
    log_names = log_names                # Filter scenarios by log names
    map_names = None                     # Filter scenarios by map names

    num_scenarios_per_type               # Number of scenarios per type
    limit_total_scenarios                # Limit total scenarios (float = fraction, int = num) - this filter can be applied on top of num_scenarios_per_type
    timestamp_threshold_s = None         # Filter scenarios to ensure scenarios have more than `timestamp_threshold_s` seconds between their initial lidar timestamps
    ego_displacement_minimum_m = None    # Whether to remove scenarios where the ego moves less than a certain amount

    expand_scenarios = True              # Whether to expand multi-sample scenarios to multiple single-sample scenarios
    remove_invalid_goals = False          # Whether to remove scenarios where the mission goal is invalid
    shuffle                              # Whether to shuffle the scenarios

    ego_start_speed_threshold = None     # Limit to scenarios where the ego reaches a certain speed from below
    ego_stop_speed_threshold = None      # Limit to scenarios where the ego reaches a certain speed from above
    speed_noise_tolerance = None         # Value at or below which a speed change between two timepoints should be ignored as noise.

    return scenario_types, scenario_tokens, log_names, map_names, num_scenarios_per_type, limit_total_scenarios, timestamp_threshold_s, ego_displacement_minimum_m, \
           expand_scenarios, remove_invalid_goals, shuffle, ego_start_speed_threshold, ego_stop_speed_threshold, speed_noise_tolerance

def process_scenario_batch(scenarios_batch, args, batch_id):
    """
    å¤„ç†ä¸€æ‰¹åœºæ™¯çš„å‡½æ•°ï¼Œç”¨äºå¤šè¿›ç¨‹æ‰§è¡Œ
    """
    processor = DataProcessor(args)
    results = []
    
    # ä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ›å»ºè¿›åº¦æ¡
    batch_pbar = tqdm(scenarios_batch, 
                     desc=f"æ‰¹æ¬¡ {batch_id}", 
                     position=batch_id,
                     leave=False,
                     ncols=100)
    
    for scenario in batch_pbar:
        try:
            # å•ç‹¬å¤„ç†æ¯ä¸ªåœºæ™¯
            map_name = scenario._map_name
            token = scenario.token
            result = processor.process_single_scenario(scenario)
            if result is not None:
                processor.save_to_disk(args.save_path, result)
                results.append(f"{map_name}_{token}.npz")
                
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            batch_pbar.set_postfix({
                'å·²å®Œæˆ': len(results),
                'æˆåŠŸç‡': f"{len(results)/len(scenarios_batch)*100:.1f}%"
            })
            
        except Exception as e:
            print(f"å¤„ç†åœºæ™¯æ—¶å‡ºé”™ {scenario.token}: {str(e)}")
            continue
    
    batch_pbar.close()
    return results, batch_id

def split_scenarios_into_batches(scenarios, num_workers):
    """
    å°†åœºæ™¯åˆ—è¡¨åˆ†å‰²æˆæ‰¹æ¬¡ä¾›å¤šè¿›ç¨‹å¤„ç†
    """
    batch_size = max(1, len(scenarios) // num_workers)
    batches = []
    
    for i in range(0, len(scenarios), batch_size):
        batch = scenarios[i:i + batch_size]
        batches.append(batch)
    
    return batches

def print_progress_summary(completed_batches, total_batches, total_scenarios, start_time):
    """
    æ‰“å°è¿›åº¦æ‘˜è¦
    """
    elapsed_time = time.time() - start_time
    progress_pct = completed_batches / total_batches * 100
    
    print(f"\n{'='*60}")
    print(f"è¿›åº¦æ‘˜è¦:")
    print(f"  å·²å®Œæˆæ‰¹æ¬¡: {completed_batches}/{total_batches} ({progress_pct:.1f}%)")
    print(f"  å·²ç”¨æ—¶é—´: {elapsed_time:.1f}ç§’")
    
    if completed_batches > 0:
        avg_time_per_batch = elapsed_time / completed_batches
        estimated_total_time = avg_time_per_batch * total_batches
        remaining_time = estimated_total_time - elapsed_time
        print(f"  é¢„è®¡å‰©ä½™æ—¶é—´: {remaining_time:.1f}ç§’")
        print(f"  é¢„è®¡æ€»æ—¶é—´: {estimated_total_time:.1f}ç§’")
    
    print(f"{'='*60}\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Data Processing')
    parser.add_argument('--data_path', default='/data/nuplan-v1.1/trainval', type=str, help='path to raw data')
    parser.add_argument('--map_path', default='/data/nuplan-v1.1/maps', type=str, help='path to map data')

    parser.add_argument('--save_path', default='./cache', type=str, help='path to save processed data')
    parser.add_argument('--scenarios_per_type', type=int, default=None, help='number of scenarios per type')
    parser.add_argument('--total_scenarios', type=int, default=10, help='limit total number of scenarios')
    parser.add_argument('--shuffle_scenarios', type=bool, default=True, help='shuffle scenarios')

    parser.add_argument('--agent_num', type=int, help='number of agents', default=32)
    parser.add_argument('--static_objects_num', type=int, help='number of static objects', default=5)

    parser.add_argument('--lane_len', type=int, help='number of lane point', default=20)
    parser.add_argument('--lane_num', type=int, help='number of lanes', default=70)

    parser.add_argument('--route_len', type=int, help='number of route lane point', default=20)
    parser.add_argument('--route_num', type=int, help='number of route lanes', default=25)
    
    # æ–°å¢å¤šè¿›ç¨‹ç›¸å…³å‚æ•°
    parser.add_argument('--num_workers', type=int, default=mp.cpu_count()//2, 
                       help='number of worker processes for parallel processing')
    parser.add_argument('--use_multiprocessing', action='store_true', 
                       help='enable multiprocessing for faster data processing')
    parser.add_argument('--show_detailed_progress', action='store_true', default=True,
                       help='show detailed progress information')
    
    args = parser.parse_args()

    print(f"ğŸš€ å¼€å§‹æ•°æ®å¤„ç†...")
    print(f"ğŸ“ æ•°æ®è·¯å¾„: {args.data_path}")
    print(f"ğŸ—ºï¸  åœ°å›¾è·¯å¾„: {args.map_path}")
    print(f"ğŸ’¾ ä¿å­˜è·¯å¾„: {args.save_path}")
    print(f"ğŸ¯ ç›®æ ‡åœºæ™¯æ•°: {args.total_scenarios}")
    
    if args.use_multiprocessing:
        print(f"âš¡ å¤šè¿›ç¨‹æ¨¡å¼: å¯ç”¨ ({args.num_workers} ä¸ªè¿›ç¨‹)")
    else:
        print(f"ğŸŒ å•è¿›ç¨‹æ¨¡å¼: å¯ç”¨")
    print(f"{'='*60}")

    # create save folder
    os.makedirs(args.save_path, exist_ok=True)

    sensor_root = None
    db_files = None

    # Only preprocess the training data
    print("ğŸ“– åŠ è½½è®­ç»ƒæ•°æ®é…ç½®...")
    with open('./nuplan_train.json', "r", encoding="utf-8") as file:
        log_names = json.load(file)

    print("ğŸ—ï¸  æ„å»ºåœºæ™¯...")
    map_version = "nuplan-maps-v1.0"    
    builder = NuPlanScenarioBuilder(args.data_path, args.map_path, sensor_root, db_files, map_version)
    scenario_filter = ScenarioFilter(*get_filter_parameters(args.scenarios_per_type, args.total_scenarios, args.shuffle_scenarios, log_names=log_names))

    worker = SingleMachineParallelExecutor(use_process_pool=True)
    scenarios = builder.get_scenarios(scenario_filter, worker)
    print(f"âœ… åœºæ™¯åŠ è½½å®Œæˆï¼Œæ€»æ•°: {len(scenarios)}")

    # process data
    del worker, builder, scenario_filter
    
    start_time = time.time()
    
    if args.use_multiprocessing and len(scenarios) > 1:
        print(f"\nâš¡ å¯åŠ¨å¤šè¿›ç¨‹å¤„ç† (è¿›ç¨‹æ•°: {args.num_workers})")
        
        # å°†åœºæ™¯åˆ†å‰²æˆæ‰¹æ¬¡
        scenario_batches = split_scenarios_into_batches(scenarios, args.num_workers)
        print(f"ğŸ“¦ åœºæ™¯åˆ†ä¸º {len(scenario_batches)} ä¸ªæ‰¹æ¬¡")
        
        # æ˜¾ç¤ºæ‰¹æ¬¡ä¿¡æ¯
        for i, batch in enumerate(scenario_batches):
            print(f"   æ‰¹æ¬¡ {i+1}: {len(batch)} ä¸ªåœºæ™¯")
        print()
        
        all_npz_files = []
        completed_batches = 0
        
        # åˆ›å»ºæ€»è¿›åº¦æ¡
        total_pbar = tqdm(total=len(scenarios), 
                         desc="æ€»è¿›åº¦", 
                         position=len(scenario_batches),
                         ncols=100,
                         bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')
        
        # ä½¿ç”¨è¿›ç¨‹æ± æ‰§è¡Œå™¨
        with ProcessPoolExecutor(max_workers=args.num_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            futures = {}
            for i, batch in enumerate(scenario_batches):
                future = executor.submit(process_scenario_batch, batch, args, i+1)
                futures[future] = i+1
                
            print(f"ğŸ“¤ å·²æäº¤ {len(futures)} ä¸ªæ‰¹æ¬¡ä»»åŠ¡\n")
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                try:
                    batch_results, batch_id = future.result()
                    all_npz_files.extend(batch_results)
                    completed_batches += 1
                    
                    # æ›´æ–°æ€»è¿›åº¦æ¡
                    total_pbar.update(len(batch_results))
                    total_pbar.set_postfix({
                        'æ‰¹æ¬¡': f"{completed_batches}/{len(scenario_batches)}",
                        'æˆåŠŸ': len(all_npz_files),
                        'æˆåŠŸç‡': f"{len(all_npz_files)/len(scenarios)*100:.1f}%"
                    })
                    
                    # æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
                    if args.show_detailed_progress:
                        elapsed = time.time() - start_time
                        avg_time = elapsed / len(all_npz_files) if all_npz_files else 0
                        print(f"\nâœ… æ‰¹æ¬¡ {batch_id} å®Œæˆ: {len(batch_results)} ä¸ªåœºæ™¯")
                        print(f"   ç´¯è®¡å®Œæˆ: {len(all_npz_files)}/{len(scenarios)} ({len(all_npz_files)/len(scenarios)*100:.1f}%)")
                        print(f"   å¹³å‡è€—æ—¶: {avg_time:.3f}ç§’/åœºæ™¯")
                        
                except Exception as e:
                    print(f"âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {str(e)}")
        
        total_pbar.close()
        npz_files = all_npz_files
        
    else:
        print("\nğŸŒ ä½¿ç”¨å•è¿›ç¨‹å¤„ç†")
        processor = DataProcessor(args)
        
        # å•è¿›ç¨‹ä¹Ÿæ˜¾ç¤ºè¿›åº¦æ¡
        scenarios_pbar = tqdm(scenarios, desc="å¤„ç†åœºæ™¯", ncols=100)
        processed_files = []
        
        for scenario in scenarios_pbar:
            result = processor.process_single_scenario(scenario)
            if result is not None:
                processor.save_to_disk(args.save_path, result)
                processed_files.append(f"{result['map_name']}_{result['token']}.npz")
            
            scenarios_pbar.set_postfix({
                'å·²å®Œæˆ': len(processed_files),
                'æˆåŠŸç‡': f"{len(processed_files)/(scenarios_pbar.n+1)*100:.1f}%"
            })
        
        npz_files = [f for f in os.listdir(args.save_path) if f.endswith('.npz')]

    end_time = time.time()
    processing_time = end_time - start_time
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"ğŸ‰ æ•°æ®å¤„ç†å®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {processing_time:.2f} ç§’")
    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   - ç›®æ ‡åœºæ™¯æ•°: {len(scenarios)}")
    print(f"   - æˆåŠŸå¤„ç†: {len(npz_files)}")
    print(f"   - æˆåŠŸç‡: {len(npz_files)/len(scenarios)*100:.1f}%")
    print(f"   - å¹³å‡è€—æ—¶: {processing_time/len(scenarios):.3f} ç§’/åœºæ™¯")
    
    if args.use_multiprocessing:
        speedup = len(scenarios) / processing_time if processing_time > 0 else 0
        theoretical_speedup = args.num_workers
        efficiency = speedup / theoretical_speedup * 100 if theoretical_speedup > 0 else 0
        print(f"âš¡ å¹¶è¡Œæ•ˆç‡:")
        print(f"   - ä½¿ç”¨è¿›ç¨‹æ•°: {args.num_workers}")
        print(f"   - å®é™…åŠ é€Ÿæ¯”: {speedup:.1f}x")
        print(f"   - å¹¶è¡Œæ•ˆç‡: {efficiency:.1f}%")
    
    print(f"{'='*60}")

    # Save the list to a JSON file
    with open('./diffusion_planner_training.json', 'w') as json_file:
        json.dump(npz_files, json_file, indent=4)

    print(f"ğŸ’¾ å·²ä¿å­˜ {len(npz_files)} ä¸ªæ–‡ä»¶ååˆ° diffusion_planner_training.json")